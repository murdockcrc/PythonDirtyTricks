{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python362jvsc74a57bd007e157dbfa446fba5f54afc6ccc42365c5fb2662ae7c0a959332e3f524ce394a",
   "display_name": "Python 3.6.2 64-bit ('azureml_rl': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Failure while loading azureml_run_type_providers. Failed to load entrypoint automl = azureml.train.automl.run:AutoMLRun._from_run_dto with exception unknown locale: UTF-8.\n"
     ]
    }
   ],
   "source": [
    "# Azure ML Core imports\n",
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.runconfig import EnvironmentDefinition\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.tensorboard import Tensorboard\n",
    "\n",
    "# Azure ML Reinforcement Learning imports\n",
    "from azureml.contrib.train.rl import ReinforcementLearningEstimator, Ray\n",
    "from azureml.contrib.train.rl import WorkerConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Warning: Falling back to use azure cli login credentials.\nIf you run your code in unattended mode, i.e., where you can't give a user input, then we recommend to use ServicePrincipalAuthentication or MsiAuthentication.\nPlease refer to aka.ms/aml-notebook-auth for different authentication mechanisms in azureml-sdk.\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "vnet_name = 'luisdel-ml-vnet'"
   ]
  },
  {
   "source": [
    "# Create Reinforcement Learning Experiment"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name='rllib-pong-multi-node'\n",
    "\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "source": [
    "# RL and Virtual Network\n",
    "\n",
    "For RL jobs that use multiple compute targets, you must specify a virtual network with open ports that allow worker nodes and head nodes to communicate with each other.\n",
    "The virtual network can be in any resource group, but it should be in the same region as your workspace.\n",
    "\n",
    "This script does not create the vnet. It must already exist, with a subnet called 'default'"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Create compute targets (head and workers)\n",
    "\n",
    "## Create head compute"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "found head compute target. just use it head-gpu\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute, ComputeTarget\n",
    "\n",
    "# choose a name for the Ray head cluster\n",
    "head_compute_name = 'head-gpu'\n",
    "head_compute_min_nodes = 0\n",
    "head_compute_max_nodes = 2\n",
    "\n",
    "# This example uses GPU VM. For using CPU VM, set SKU to STANDARD_D2_V2\n",
    "head_vm_size = 'STANDARD_NC6'\n",
    "\n",
    "if head_compute_name in ws.compute_targets:\n",
    "    head_compute_target = ws.compute_targets[head_compute_name]\n",
    "    if head_compute_target and type(head_compute_target) is AmlCompute:\n",
    "        print(f'found head compute target. just use it {head_compute_name}')\n",
    "else:\n",
    "    print('creating a new head compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = head_vm_size,\n",
    "                                                                min_nodes = head_compute_min_nodes, \n",
    "                                                                max_nodes = head_compute_max_nodes,\n",
    "                                                                vnet_resourcegroup_name = ws.resource_group,\n",
    "                                                                idle_seconds_before_scaledown=600,\n",
    "                                                                vm_priority='lowpriority',\n",
    "                                                                vnet_name = vnet_name,\n",
    "                                                                subnet_name = 'default')\n",
    "\n",
    "    # create the cluster\n",
    "    head_compute_target = ComputeTarget.create(ws, head_compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    head_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(head_compute_target.get_status().serialize())"
   ]
  },
  {
   "source": [
    "## Create worker compute"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "found worker compute target. just use it worker-cpu\n"
     ]
    }
   ],
   "source": [
    "# choose a name for your Ray worker cluster\n",
    "worker_compute_name = 'worker-cpu'\n",
    "worker_compute_min_nodes = 0 \n",
    "worker_compute_max_nodes = 4\n",
    "\n",
    "# This example uses CPU VM. For using GPU VM, set SKU to STANDARD_NC6\n",
    "worker_vm_size = 'STANDARD_D2_V2'\n",
    "\n",
    "# Create the compute target if it hasn't been created already\n",
    "if worker_compute_name in ws.compute_targets:\n",
    "    worker_compute_target = ws.compute_targets[worker_compute_name]\n",
    "    if worker_compute_target and type(worker_compute_target) is AmlCompute:\n",
    "        print(f'found worker compute target. just use it {worker_compute_name}')\n",
    "else:\n",
    "    print('creating a new worker compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = worker_vm_size,\n",
    "                                                                min_nodes = worker_compute_min_nodes, \n",
    "                                                                max_nodes = worker_compute_max_nodes,\n",
    "                                                                vnet_resourcegroup_name = ws.resource_group,\n",
    "                                                                idle_seconds_before_scaledown=600,\n",
    "                                                                vm_priority='lowpriority',\n",
    "                                                                vnet_name = vnet_name,\n",
    "                                                                subnet_name = 'default')\n",
    "\n",
    "    # create the cluster\n",
    "    worker_compute_target = ComputeTarget.create(ws, worker_compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    worker_compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current AmlCompute status, use get_status()\n",
    "    print(worker_compute_target.get_status().serialize())"
   ]
  },
  {
   "source": [
    "# Prepare experiment\n",
    "\n",
    "## Define worker configuration"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_packages=[\"ray[rllib]==0.8.3\"]\n",
    "\n",
    "worker_conf = WorkerConfiguration(\n",
    "    \n",
    "    # Azure ML compute cluster to run Ray workers\n",
    "    compute_target=worker_compute_target, \n",
    "    \n",
    "    # Number of worker nodes\n",
    "    node_count=4,\n",
    "    \n",
    "    # GPU\n",
    "    use_gpu=False, \n",
    "    \n",
    "    # PIP packages to use\n",
    "    pip_packages=pip_packages\n",
    ")\n",
    "\n",
    "training_algorithm = \"IMPALA\"\n",
    "rl_environment = \"PongNoFrameskip-v4\"\n",
    "\n",
    "# Training script parameters\n",
    "script_params = {\n",
    "    \n",
    "    # Training algorithm, IMPALA in this case\n",
    "    \"--run\": training_algorithm,\n",
    "    \n",
    "    # Environment, Pong in this case\n",
    "    \"--env\": rl_environment,\n",
    "    \n",
    "    # Add additional single quotes at the both ends of string values as we have spaces in the \n",
    "    # string parameters, outermost quotes are not passed to scripts as they are not actually part of string\n",
    "    # Number of GPUs\n",
    "    # Number of ray workers\n",
    "    \"--config\": '\\'{\"num_gpus\": 1, \"num_workers\": 13}\\'',\n",
    "    \n",
    "    # Target episode reward mean to stop the training\n",
    "    # Total training time in seconds\n",
    "    \"--stop\": '\\'{\"episode_reward_mean\": 18, \"time_total_s\": 3600}\\'',\n",
    "}"
   ]
  },
  {
   "source": [
    "## Define the reinforcement learning estimator"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n",
      "'shm_size' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object instead.\n",
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n"
     ]
    }
   ],
   "source": [
    "# RL estimator\n",
    "rl_estimator = ReinforcementLearningEstimator(\n",
    "    \n",
    "    # Location of source files\n",
    "    source_directory='azure_job',\n",
    "    \n",
    "    # Python script file\n",
    "    entry_script=\"pong_rllib.py\",\n",
    "    \n",
    "    # Parameters to pass to the script file\n",
    "    # Defined above.\n",
    "    script_params=script_params,\n",
    "    \n",
    "    # The Azure ML compute target set up for Ray head nodes\n",
    "    compute_target=head_compute_target,\n",
    "    \n",
    "    # Pip packages\n",
    "    pip_packages=pip_packages,\n",
    "    \n",
    "    # GPU usage\n",
    "    use_gpu=True,\n",
    "    \n",
    "    # RL framework. Currently must be Ray.\n",
    "    rl_framework=Ray(),\n",
    "    \n",
    "    # Ray worker configuration defined above.\n",
    "    worker_configuration=worker_conf,\n",
    "    \n",
    "    # How long to wait for whole cluster to start\n",
    "    cluster_coordination_timeout_seconds=3600,\n",
    "    \n",
    "    # Maximum time for the whole Ray job to run\n",
    "    # This will cut off the run after an hour\n",
    "    max_run_duration_seconds=3600,\n",
    "    \n",
    "    # Allow the docker container Ray runs in to make full use\n",
    "    # of the shared memory available from the host OS.\n",
    "    shm_size=24*1024*1024*1024\n",
    ")"
   ]
  },
  {
   "source": [
    "## The entry script"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "azure_job_folder = os.path.join(os.getcwd(), 'azure_job')\n",
    "os.makedirs(azure_job_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Overwriting ./azure_job/pong_rllib.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./azure_job/pong_rllib.py\n",
    "\n",
    "import ray\n",
    "import ray.tune as tune\n",
    "from ray.rllib import train\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from azureml.core import Run\n",
    "\n",
    "DEFAULT_RAY_ADDRESS = 'localhost:6379'\n",
    "\n",
    "'''RLlib callbacks module:\n",
    "    Common callback methods to be passed to RLlib trainer.\n",
    "'''\n",
    "\n",
    "def on_train_result(info):\n",
    "    '''Callback on train result to record metrics returned by trainer.\n",
    "    '''\n",
    "    run = Run.get_context()\n",
    "    run.log(\n",
    "        name='episode_reward_mean',\n",
    "        value=info[\"result\"][\"episode_reward_mean\"])\n",
    "    run.log(\n",
    "        name='episodes_total',\n",
    "        value=info[\"result\"][\"episodes_total\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Parse arguments\n",
    "    train_parser = train.create_parser()\n",
    "\n",
    "    args = train_parser.parse_args()\n",
    "    print(\"Algorithm config:\", args.config)\n",
    "\n",
    "    if args.ray_address is None:\n",
    "        args.ray_address = DEFAULT_RAY_ADDRESS\n",
    "\n",
    "    ray.init(address=args.ray_address)\n",
    "\n",
    "    tune.run(run_or_experiment=args.run,\n",
    "             config={\n",
    "                 \"env\": args.env,\n",
    "                 \"num_gpus\": args.config[\"num_gpus\"],\n",
    "                 \"num_workers\": args.config[\"num_workers\"],\n",
    "                 \"callbacks\": {\"on_train_result\": on_train_result},\n",
    "                 \"sample_batch_size\": 50,\n",
    "                 \"train_batch_size\": 1000,\n",
    "                 \"num_sgd_iter\": 2,\n",
    "                 \"num_data_loader_buffers\": 2,\n",
    "                 \"model\": {\n",
    "                    \"dim\": 42\n",
    "                 },\n",
    "             },\n",
    "             stop=args.stop,\n",
    "             local_dir='./logs')"
   ]
  },
  {
   "source": [
    "## Submit a run"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n",
      "'shm_size' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object instead.\n"
     ]
    }
   ],
   "source": [
    "run = exp.submit(config=rl_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}