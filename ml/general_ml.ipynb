{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning sweet spots\n",
    "\n",
    "As of 2020, these are the types of problems DL is good at:\n",
    "\n",
    "## Computer vision\n",
    "\n",
    "Caveat: models dont perform well for an image which is not similar to those used in the training set. For example, trying to make a prediction on a black and white image, but your training data consisted only of color images.\n",
    "\n",
    "Data augmentation might help with the problem by artificially creating new training instances (ie: rotating images, changing them to black and white, etc)\n",
    "\n",
    "# Data preparation\n",
    "\n",
    "## Structuring your experiment data ##\n",
    "\n",
    "Always divide your data into the following blocks:\n",
    "\n",
    "* Split your data into a _training set_ and a _test set_\n",
    "* Split the training set further into _training set_ and a _validation_ set\n",
    "\n",
    "When evaluating the correctness of your results, always validate with the **validation** set. Do not use the test set until you think your model is ready for prime time.\n",
    "\n",
    "## Considerations for splitting data\n",
    "\n",
    "### Set aside a validation set and a test set\n",
    "\n",
    "The idea is for the validation set to me used to tune the model. However, the tunning process must never see the test set you set apart. This is to ensure that you are not indirectly overfitting the model by looking too often at this last bastion of data. For example: if you have a vendor doing a model for you, set aside a test set for yourself, which the vendor never gets to see. Once the vendor's model is finished, use this test set to control the performance according to your own metrics.\n",
    "\n",
    "### Random shuffling\n",
    "\n",
    "If you have a data set ordered in classes, ensure you do random shuffling. I.e: if you have a dataset representing single digits (classes 0-9) and you split your data 80/20 without random shuffling, your training set will not have instances of classes 8-9.\n",
    "\n",
    "### Time series\n",
    "\n",
    "Do not do random shuffling if you have time series data. Otherwise, the progression of time will be lost\n",
    "\n",
    "### Data redundancy\n",
    "\n",
    "Remove data dups. If you shuffle your data and it das dups, these dups might end up in both the test, validation and training sets.\n",
    "\n",
    "### Data representation\n",
    "\n",
    "Think about creative ways in which your data can be re-represented, so that it is easier for a model to analyze. For example, sound can be transformed into an image by creating its spectrogram. So your sound analysis problem suddenly becomes a computer vision problem.\n",
    "\n",
    "## K-fold Cross-validation\n",
    "\n",
    "If your data set is too small, splitting the data set might cause your sets to loose statistical significance. In this case, use the K-fold approach to split the data into _x_ folds, each of which will be evaluated individually and the result of those runs averaged to produce a more solid result.\n",
    "\n",
    "\\begin{align}\n",
    "CV_{(k)} = \\frac{1}{k} \\sum_{i=1}^k Loss_{i}\n",
    "\\end{align}\n",
    "\n",
    "* _k_ is the number of folds, typically 10\n",
    "* _Loss_ function typically is mean squared errors, log-loss or accuracy\n",
    "\n",
    "## Epochs\n",
    "\n",
    "This is how many times the model will see each data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing\n",
    "\n",
    "## Vectorization\n",
    "\n",
    "All your data must be expressed as tensors (either floating point numbers, or ints).\n",
    "\n",
    "## Data normalization\n",
    "\n",
    "Features must have values which have similar ranges. Typically, you will try to have all input and output values to have a range of 0-1. As a recommendations, values should have mean 0 and standard deviation 1:\n",
    "\n",
    "For a 2D matrix:\n",
    "```\n",
    "x -= x.mean(axis=0)\n",
    "x /= x.std(axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling overfitting\n",
    "\n",
    "A model should _generalize_ well. After a certain number of training iterations, the model will start learning things which are particular to the training set but not general. To deal with overfitting:\n",
    "\n",
    "* Get more training data\n",
    "* Invest in regularization\n",
    "\n",
    "## Regularization\n",
    "\n",
    "Impose constraints to the network. These constraints will make it more difficult for the network to learn particularities of the training set.\n",
    "\n",
    "### Reduce the size of the network\n",
    "\n",
    "Reduce number of layers and number of units per layer. Recommendation: start with fewer layers and parameters, and start making your architecture more complex until you get to a point of diminishing returns.\n",
    "\n",
    "### Weight regularization\n",
    "\n",
    "Impose a penalty for having large weights in the network, thus forcing the weights across the network to be more regular. This is done by adding to the loss function so that it penalizes larger weights.\n",
    "\n",
    "### Dropout\n",
    "\n",
    "Set some of the values in the output vector to zero, at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
